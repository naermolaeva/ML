{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "Delelop language model, which generates death metal band names.  \n",
    "You can get data from https://www.kaggle.com/zhangjuefei/death-metal.  \n",
    "You are free to use any other data, but the most easy way is just to take the band name column.\n",
    "\n",
    "Your language model should be char-based autogression RNN.  \n",
    "Text generation should be terminated when either max length is reached or terminal symbol is generated.  \n",
    "\n",
    "Different band names can be generated by:  \n",
    "1. init $h_0$ as random vector from some probabilty distribution.\n",
    "2. sampling over tokens at each timestep with probability = softmax \n",
    "\n",
    "Calculate perplexity for your model = your objective quality metric.  \n",
    "Also, sample 10 band names from your model for subjective evaluation. E.g. names like 'qwiouefiou23riop2h3' or 'death death death!' are bad examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qml7Iv9sKwYG"
   },
   "source": [
    "### This character level RNN is based on https://github.com/spro/char-rnn.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOGCa5JDK17n"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import autograd\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ' '.join(data['name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    # turning a string into a tensor\n",
    "    \n",
    "    tensor = tt.zeros(len(string)).long()\n",
    "    \n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "52KWKp--LRVH",
    "outputId": "12693973-3d5e-4aa3-d68b-2a108a86d347"
   },
   "outputs": [],
   "source": [
    "def random_training_set(chunk_len, batch_size, text):\n",
    "    # get the training data\n",
    "\n",
    "    inpt = tt.LongTensor(batch_size, chunk_len)\n",
    "    trgt = tt.LongTensor(batch_size, chunk_len)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "\n",
    "        inpt[i] = char_tensor(chunk[:-1])\n",
    "        trgt[i] = char_tensor(chunk[1:])\n",
    "\n",
    "    inpt = tt.autograd.Variable(inpt)\n",
    "    trgt = tt.autograd.Variable(trgt)\n",
    "\n",
    "    return inpt, trgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uw7E1yvHPbMD"
   },
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YM-RJy9gNcT3"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # init the meta parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # first lstm layer\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "          \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        encoded = self.embedding(x)\n",
    "        \n",
    "        out_x, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        out_x = self.fc(out_x.view(batch_size, -1))\n",
    "        \n",
    "        return out_x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        \n",
    "        return (tt.autograd.Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                tt.autograd.Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "hidden_size = 100\n",
    "batch_size = 512\n",
    "n_layers = 2\n",
    "chunk_len = 200\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDV_0GRbLtXw"
   },
   "outputs": [],
   "source": [
    "model = MyModel(len(characters),\n",
    "                hidden_size,\n",
    "                len(characters),\n",
    "                n_layers=n_layers\n",
    "               )\n",
    "\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(x):\n",
    "    return 2**x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_x, target_x):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    all_pp = []\n",
    "\n",
    "    for i in range(chunk_len):        \n",
    "        output, hidden = model(input_x[:,i], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target_x[:,i])\n",
    "        \n",
    "        all_pp.append(perplexity(loss.item()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return np.mean(all_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(input_x, target_x):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    all_pp = []\n",
    "\n",
    "    for i in range(chunk_len):        \n",
    "        output, hidden = model(input_x[:,i], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target_x[:,i])\n",
    "        \n",
    "        all_pp.append(perplexity(loss.item()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return np.mean(all_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8M-FSO-TTLF"
   },
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='', predict_len=10, temperature=0.8, mode='one'):\n",
    "    hidden = model.init_hidden(1)\n",
    "    prime_input = tt.autograd.Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "    predicted = str()\n",
    "\n",
    "    # use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = tt.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # add predicted character to string and use as next input\n",
    "        predicted_char = characters[top_i]\n",
    "        \n",
    "        if mode: # one-word names\n",
    "            if predicted and ' ' in predicted_char:\n",
    "                break\n",
    "            \n",
    "        predicted += predicted_char\n",
    "        inp = tt.autograd.Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46e848d3faf40999fd9f55ac3b0adf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 10 | perplexity: 9.84716 |\n",
      "| Epoch: 20 | perplexity: 8.51526 |\n",
      "| Epoch: 30 | perplexity: 7.15025 |\n",
      "| Epoch: 40 | perplexity: 6.48868 |\n",
      "| Epoch: 50 | perplexity: 6.12082 |\n",
      "| Epoch: 60 | perplexity: 5.99019 |\n",
      "| Epoch: 70 | perplexity: 5.75783 |\n",
      "| Epoch: 80 | perplexity: 5.47551 |\n",
      "| Epoch: 90 | perplexity: 5.49966 |\n",
      "| Epoch: 100 | perplexity: 5.44302 |\n",
      "| Epoch: 110 | perplexity: 5.29318 |\n",
      "| Epoch: 120 | perplexity: 5.22514 |\n",
      "| Epoch: 130 | perplexity: 5.17873 |\n",
      "| Epoch: 140 | perplexity: 5.09204 |\n",
      "| Epoch: 150 | perplexity: 5.07527 |\n",
      "| Epoch: 160 | perplexity: 5.01334 |\n",
      "| Epoch: 170 | perplexity: 4.97672 |\n",
      "| Epoch: 180 | perplexity: 4.99073 |\n",
      "| Epoch: 190 | perplexity: 4.97100 |\n",
      "| Epoch: 200 | perplexity: 4.85161 |\n",
      "| Epoch: 210 | perplexity: 4.85657 |\n",
      "| Epoch: 220 | perplexity: 4.84165 |\n",
      "| Epoch: 230 | perplexity: 4.75802 |\n",
      "| Epoch: 240 | perplexity: 4.75274 |\n",
      "| Epoch: 250 | perplexity: 4.74760 |\n",
      "| Epoch: 260 | perplexity: 4.73505 |\n",
      "| Epoch: 270 | perplexity: 4.65666 |\n",
      "| Epoch: 280 | perplexity: 4.60291 |\n",
      "| Epoch: 290 | perplexity: 4.62963 |\n",
      "| Epoch: 300 | perplexity: 4.59348 |\n",
      "| Epoch: 310 | perplexity: 4.54202 |\n",
      "| Epoch: 320 | perplexity: 4.60633 |\n",
      "| Epoch: 330 | perplexity: 4.54708 |\n",
      "| Epoch: 340 | perplexity: 4.56429 |\n",
      "| Epoch: 350 | perplexity: 4.54719 |\n",
      "| Epoch: 360 | perplexity: 4.54773 |\n",
      "| Epoch: 370 | perplexity: 4.52633 |\n",
      "| Epoch: 380 | perplexity: 4.51014 |\n",
      "| Epoch: 390 | perplexity: 4.51708 |\n",
      "| Epoch: 400 | perplexity: 4.47116 |\n",
      "| Epoch: 410 | perplexity: 4.51004 |\n",
      "| Epoch: 420 | perplexity: 4.40837 |\n",
      "| Epoch: 430 | perplexity: 4.39658 |\n",
      "| Epoch: 440 | perplexity: 4.48716 |\n",
      "| Epoch: 450 | perplexity: 4.40085 |\n",
      "| Epoch: 460 | perplexity: 4.34392 |\n",
      "| Epoch: 490 | perplexity: 4.43718 |\n",
      "| Epoch: 500 | perplexity: 4.34924 |\n",
      "| Epoch: 510 | perplexity: 4.33872 |\n",
      "| Epoch: 520 | perplexity: 4.28850 |\n",
      "| Epoch: 530 | perplexity: 4.32157 |\n",
      "| Epoch: 540 | perplexity: 4.28532 |\n",
      "| Epoch: 550 | perplexity: 4.35441 |\n",
      "| Epoch: 560 | perplexity: 4.36540 |\n",
      "| Epoch: 570 | perplexity: 4.28792 |\n",
      "| Epoch: 580 | perplexity: 4.27440 |\n",
      "| Epoch: 590 | perplexity: 4.29209 |\n",
      "| Epoch: 600 | perplexity: 4.30312 |\n",
      "| Epoch: 610 | perplexity: 4.26888 |\n",
      "| Epoch: 620 | perplexity: 4.24308 |\n",
      "| Epoch: 630 | perplexity: 4.25865 |\n",
      "| Epoch: 640 | perplexity: 4.20601 |\n",
      "| Epoch: 650 | perplexity: 4.23336 |\n",
      "| Epoch: 660 | perplexity: 4.17464 |\n",
      "| Epoch: 670 | perplexity: 4.25291 |\n",
      "| Epoch: 680 | perplexity: 4.19460 |\n",
      "| Epoch: 690 | perplexity: 4.23939 |\n",
      "| Epoch: 700 | perplexity: 4.18658 |\n",
      "| Epoch: 710 | perplexity: 4.14067 |\n",
      "| Epoch: 720 | perplexity: 4.21372 |\n",
      "| Epoch: 730 | perplexity: 4.19528 |\n",
      "| Epoch: 740 | perplexity: 4.13147 |\n",
      "| Epoch: 750 | perplexity: 4.11741 |\n",
      "| Epoch: 760 | perplexity: 4.16432 |\n",
      "| Epoch: 770 | perplexity: 4.15507 |\n",
      "| Epoch: 780 | perplexity: 4.14344 |\n",
      "| Epoch: 790 | perplexity: 4.13957 |\n",
      "| Epoch: 800 | perplexity: 4.16017 |\n",
      "| Epoch: 810 | perplexity: 4.17121 |\n",
      "| Epoch: 820 | perplexity: 4.11870 |\n",
      "| Epoch: 830 | perplexity: 4.13030 |\n",
      "| Epoch: 840 | perplexity: 4.14475 |\n",
      "| Epoch: 850 | perplexity: 4.10616 |\n",
      "| Epoch: 860 | perplexity: 4.13609 |\n",
      "| Epoch: 870 | perplexity: 4.11390 |\n",
      "| Epoch: 880 | perplexity: 4.05094 |\n",
      "| Epoch: 890 | perplexity: 4.04564 |\n",
      "| Epoch: 920 | perplexity: 4.05149 |\n",
      "| Epoch: 930 | perplexity: 4.07793 |\n",
      "| Epoch: 940 | perplexity: 4.04574 |\n",
      "| Epoch: 950 | perplexity: 4.04456 |\n",
      "| Epoch: 960 | perplexity: 4.04689 |\n",
      "| Epoch: 970 | perplexity: 4.08591 |\n",
      "| Epoch: 980 | perplexity: 4.11776 |\n",
      "| Epoch: 990 | perplexity: 4.03854 |\n",
      "| Epoch: 1000 | perplexity: 4.05109 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(1, n_epochs + 1)):\n",
    "    pp = train(*random_training_set(chunk_len, batch_size, bands))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'| Epoch: {epoch} | perplexity: {pp:.5f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflosis\n",
      "Ercubution\n",
      "Mi's\n",
      "D.a.1\n",
      "Grypt\n",
      "Ovocops\n",
      "Internal\n",
      "Funulon\n",
      "in\n",
      "Inlembium\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(model, ' ', 10, mode='one'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeAperatio\n",
      "Dark\n",
      "Fourn\n",
      "Laginged\n",
      "Reipse\n",
      "Bear\n",
      "Sungetic\n",
      "Evil\n",
      "Godhtrepse\n",
      "Taut\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(model, ' ', 10, mode='one'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sate\n",
      "Hyl.E.W.H.\n",
      "Dead\n",
      "Ettlor\n",
      "Abarzat\n",
      "Marsar\n",
      "Expents\n",
      "Untharfent\n",
      "Effitant\n",
      "Narete\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(model, ' ', 10, mode='one'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dieva Anara Mid\n",
      "Sange Santophil\n",
      "Remage Uninemin\n",
      "Noctus Herei Re\n",
      "the Widned Fung\n",
      "Tolent Massion \n",
      "Doom Sples Orat\n",
      "Repice Infernal\n",
      "Erory Vown Pole\n",
      "Infection Syce \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(model, ' ', 15, mode=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rut Horthro\n",
      "Primoted Sp\n",
      "Spone Morti\n",
      "Necrophover\n",
      "Lyin Death \n",
      "Aband A.L. \n",
      "Found Rate \n",
      "Horrent Exu\n",
      "Anas Vecrif\n",
      "Proogets Fi\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate(model, ' ', 11, mode=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    model_name = 'char-rnn-final.pt'\n",
    "    tt.save(model, model_name)\n",
    "    \n",
    "    print('saving model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model as char-rnn-final.pt\n"
     ]
    }
   ],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CharRNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
