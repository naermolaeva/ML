{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8 (unfinished)\n",
    "\n",
    "Develop a model for 20 news groups dataset. Select 20% of data for test set.  \n",
    "\n",
    "Use metric learning with siamese networks and triplet loss.   \n",
    "Use KNN and LSH (`annoy` library) for final prediction after the network was trained.\n",
    "\n",
    "! Remember, that LSH gives you a set of neighbor candidates, for which you have to calculate distances to choose top-k nearest neighbors. \n",
    "\n",
    "Your quality = accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data + train, validation, test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(newsgroups_train.data,\n",
    "                                                  newsgroups_train.target,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=SEED,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "X_test, y_test = newsgroups_test.data, newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[allennlp.modules.elmo.batch_to_ids()](https://allenai.github.io/allennlp-docs/api/allennlp.modules.elmo.html#allennlp.modules.elmo.batch_to_ids)\n",
    "\n",
    "Converts a batch of **tokenized sentences** to a tensor representing the sentences with encoded characters (len(batch), max sentence length, max word length).\n",
    "\n",
    "`batch_to_ids(batch:typing.List[typing.List[str]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['First sentence.', 'Another one sentence.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return [[tok.lemma_ for tok in spacy_en.tokenizer(sentence) if tok.text.isalpha()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['First', 'sentence']]\n",
      "[['Another', 'one', 'sentence']]\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    print(tokenizer(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[the 12th seminar](https://github.com/thedenaas/hse_seminars_2018/blob/master/seminar_12/more_embeddings.ipynb):\n",
    "\n",
    "```def branch(self, x):\n",
    "    x = self.elmo(x)['elmo_representations']\n",
    "    x = tt.cat(x, dim=-1)\n",
    "    x = x.mean(dim=1)\n",
    "    x = self.fc(x)\n",
    "    return x```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embeddings(p, sentences):\n",
    "    for sent in tqdm_notebook(sentences):\n",
    "        tok_sent = tokenizer(sent)\n",
    "        character_ids = batch_to_ids(tok_sent)\n",
    "        embeddings = elmo(character_ids)['elmo_representations']\n",
    "        x = tt.cat(embeddings, dim=-1)\n",
    "        x = x.mean(dim=1)\n",
    "        x = x.detach().numpy() # if x.numpy(): Can't call numpy() on Variable that requires grad. \n",
    "                               # Use var.detach().numpy() instead.\n",
    "        \n",
    "        with p.open('ab') as f:\n",
    "            np.save(f, x)\n",
    "\n",
    "        with p.open('rb') as f:\n",
    "            fsz = os.fstat(f.fileno()).st_size\n",
    "            out = np.load(f)\n",
    "            while f.tell() < fsz:\n",
    "                out = np.vstack((out, np.load(f)))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf160b3f8f5e4b4b881030305a9742e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = Path('test_embeddings.npy')\n",
    "test_embeddings = elmo_embeddings(test_path, X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142ab4c193814f2692e89a06bc045022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = Path('train_embeddings.npy')\n",
    "train_embeddings = elmo_embeddings(train_path, X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70657db4144d480ca651ec4b003fa930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_path = Path('valid_embeddings.npy')\n",
    "valid_embeddings = elmo_embeddings(valid_path, X_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path_full = Path('train_embeddings_full.npy')\n",
    "# train_embeddings_full = elmo_embeddings(train_path_full, X_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_path_full = Path('valid_embeddings_full.npy')\n",
    "# valid_embeddings_full = elmo_embeddings(valid_path_full, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tt, y_val_tt, y_test_tt = tt.FloatTensor(y_train), tt.FloatTensor(y_val), tt.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets(data, label):\n",
    "    \n",
    "    # positive -- the encodings for the positive data (similar to anchor) --> y[curr] == y\n",
    "    # negative -- the encodings for the negative data (different from anchor) --> y[curr] != y\n",
    "    \n",
    "    pos = []\n",
    "    neg = []\n",
    "    \n",
    "    for idx in range(len((data))):\n",
    "        positive = data[np.random.choice(np.where(label == label[idx])[0])]\n",
    "        pos.append(positive)\n",
    "        \n",
    "        negative = data[np.random.choice(np.where(label != label[idx])[0])]\n",
    "        neg.append(negative)\n",
    "\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train, neg_train = get_triplets(train_embeddings, y_train_tt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_val, neg_val = get_triplets(valid_embeddings, y_val_tt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test, neg_test = get_triplets(test_embeddings, y_test_tt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(tt.Tensor(train_embeddings), tt.FloatTensor(pos_train), tt.FloatTensor(neg_train)), batch_size=batch_size)\n",
    "val_loader = DataLoader(TensorDataset(tt.Tensor(train_embeddings), tt.FloatTensor(pos_val), tt.FloatTensor(neg_val)), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(tt.Tensor(test_embeddings), tt.FloatTensor(pos_test), tt.FloatTensor(neg_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(batch[0], batch[1], batch[2]).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss = model(batch[0], batch[1], batch[2]).sum()\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEMINAR\n",
    "\n",
    "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
    "    return F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed).mean()\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, elmo, criterion):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.elmo = elmo\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.fc = nn.Linear(1024*2, 128)\n",
    "        \n",
    "        self.out = nn.Linear(128*3, 1)\n",
    "        \n",
    "    def branch(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, anchor, pos, neg):\n",
    "        anchor = self.branch(anchor)\n",
    "        pos = self.branch(pos)\n",
    "        neg = self.branch(neg)\n",
    "        \n",
    "        return triplet_loss(anchor, pos, neg)\n",
    "\n",
    "\n",
    "\n",
    "# model = MyModel(elmo, nn.BCEWithLogitsLoss())\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# nn_train(model, train_loader, val_loader, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf30d9b08a54d27bac22bdfe54107d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.65779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f8bfb84dcc4903908fa5aa17dd0776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 1.27336\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(elmo, nn.BCEWithLogitsLoss())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nn_train(model, train_loader, val_loader, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = tt.from_numpy(train_embeddings)\n",
    "_train = model.branch(_train).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/nicw102168/nearest-neighbor-classification-with-annoy\n",
    "\n",
    "import annoy\n",
    "\n",
    "def test_annoy_knn(Ntrees=128):\n",
    "    \n",
    "    vector_length = 128\n",
    "    t = annoy.AnnoyIndex(vector_length)\n",
    "    X_train = model.branch(tt.from_numpy(train_embeddings)).detach().numpy()\n",
    "    for i, v in enumerate(X_train):\n",
    "        t.add_item(i, v)\n",
    "\n",
    "    t.build(Ntrees)\n",
    "    \n",
    "    # error with shapes\n",
    "\n",
    "#     y_hat = [y_train_tt[:5][t.get_nns_by_vector(v, 10)] for v in test_embeddings[:5]]\n",
    "#     acc = metrics.accuracy_score(y_test_tt[:5], y_hat)\n",
    "#     conf = metrics.confusion_matrix(y_test_tt[:5], y_hat)\n",
    "#     return acc, conf\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<annoy.Annoy at 0x7f0f50ebd1f0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_annoy_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model as assignment8.pt\n"
     ]
    }
   ],
   "source": [
    "def save_model():\n",
    "    model_name = 'assignment8.pt'\n",
    "    tt.save(model, model_name)\n",
    "    \n",
    "    print('saving model as %s' % model_name)\n",
    "    \n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
